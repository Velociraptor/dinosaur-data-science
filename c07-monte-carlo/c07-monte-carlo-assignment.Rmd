---
title: "Estimating Pi With a Shotgun"
author: "Hannah Sarver"
date: 2020-08-03
output:
  github_document:
    toc: true
---

*Purpose*: Random sampling is extremely powerful. To build more intuition for how we can use random sampling to solve problems, we'll tackle what---at first blush---doesn't seem appropriate for a random approach: estimating fundamental deterministic constants. In this challenge you'll work through an example of turning a deterministic problem into a random sampling problem, and practice quantifying uncertainty in your estimate.

```{r setup}
library(tidyverse)
```

*Background*: In 2014, some crazy Quebecois physicists estimated $\pi$ with a pump-action shotgun[1,2]. Their technique was based on the *Monte Carlo method*, a general strategy for turning deterministic problems into random sampling.

# Monte Carlo
<!-- -------------------------------------------------- -->

The [Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method) is the use of randomness to produce approximate answers to deterministic problems. Its power lies in its simplicity: So long as we can take our deterministic problem and express it in terms of random variables, we can use simple random sampling to produce an approximate answer. Monte Carlo has an [incredible number](https://en.wikipedia.org/wiki/Monte_Carlo_method#Applications) of applications; for instance Ken Perlin won an [Academy Award](https://en.wikipedia.org/wiki/Perlin_noise) for developing a particular flavor of Monte Carlo for generating artificial textures.

I remember when I first learned about Monte Carlo, I thought the whole idea was pretty strange: If I have a deterministic problem, why wouldn't I just "do the math" and get the right answer? It turns out "doing the math" is often hard---and in some cases an analytic solution is simply not possible. Problems that are easy to do by hand can quickly become intractable if you make a slight change to the problem formulation. Monte Carlo is a *general* approach; so long as you can model your problem in terms of random variables, you can apply the Monte Carlo method. See Ref. [3] for many more details on using Monte Carlo.

In this challenge, we'll tackle a deterministic problem (computing $\pi$) with the Monte Carlo method.

## Theory
<!-- ------------------------- -->

The idea behind estimating $\pi$ via Monte Carlo is to set up a probability estimation problem whose solution is related to $\pi$. Consider the following sets: a square with side length one $St$, and a quarter-circle $Sc$.

```{r vis-areas}
tibble(x = seq(0, 1, length.out = 100)) %>%
  mutate(y = sqrt(1 - x^2)) %>%

  ggplot(aes(x, y)) +
  annotate(
    "rect",
    xmin = 0, ymin = 0, xmax = 1, ymax = 1,
    fill = "grey40",
    size = 1
  ) +
  geom_ribbon(aes(ymin = 0, ymax = y), fill = "coral") +
  geom_line() +
  annotate(
    "label",
    x = 0.5, y = 0.5, label = "Sc",
    size = 8
  ) +
  annotate(
    "label",
    x = 0.8, y = 0.8, label = "St",
    size = 8
  ) +
  scale_x_continuous(breaks = c(0, 1/2, 1)) +
  scale_y_continuous(breaks = c(0, 1/2, 1)) +
  theme_minimal()
```

The area of the set $Sc$ is $\pi/4$, while the area of $St$ is $1$. Thus the probability that a *uniform* random variable over the square lands inside $Sc$ is the ratio of the areas, that is

$$\mathbb{P}_{X}[X \in Sc] = (\pi / 4) / 1.$$

Re-arranging, we find

$$\pi = 4 \times \mathbb{P}_{X}[X \in Sc].$$

This expression is our ticket to estimating $\pi$ with a source of randomness: If we estimate the probability above and multiply by $4$, we'll be estimating $\pi$.

## Implementation
<!-- ------------------------- -->

Remember in `e-stat02-probability` we learned how to estimate probabilities as the limit of frequencies. Use your knowledge from that exercise to generate Monte Carlo data.

__q1__ Pick a sample size $n$ and generate $n$ points *uniform randomly* in the square $x \in [0, 1]$ and $y \in [0, 1]$. Create a column `stat` whose mean will converge to $\pi$.

*Hint*: Remember that the mean of an *indicator function* on your target set will estimate the probability of points landing in that area (see `e-stat02-probability`). Based on the expression above, you'll need to *modify* that indicator to produce an estimate of $\pi$.

```{r q1-task}
## TASK: Choose a sample size and generate samples
n <- 100000 # Choose a sample size
# Generate the data
df_q1 <- tibble(
      x = runif(n, 0, 1),
      y = runif(n, 0, 1)
    ) %>%
    mutate(in_circle = (y <= sqrt(1 - x^2))) %>%
    summarize(
      count_total = n(),
      count_circle = sum(in_circle),
      fr = mean(in_circle),
      se = sd(in_circle)/sqrt(count_total),
      pi_est = fr*4
    )

df_q1
```

__q2__ Using your data in `df_q1`, estimate $\pi$.

```{r q2-task}
## TASK: Estimate pi using your data from q1
pi_est <- df_q1$pi_est
pi_est
```

# Quantifying Uncertainty
<!-- -------------------------------------------------- -->

You now have an estimate of $\pi$, but how trustworthy is that estimate? In `e-stat06-clt` we discussed *confidence intervals* as a means to quantify the uncertainty in an estimate. Now you'll apply that knowledge to assess your $\pi$ estimate.

__q3__ Using a CLT approximation, produce a confidence interval for your estimate of $\pi$. Does your interval include the true value of $\pi$? Was your chosen sample size sufficiently large so as to produce a trustworthy answer?

```{r q3-task}
# I'm actually kinda confused about how precisely to apply CLT here - should this be across a bunch of sampling iterations?
# And should the stddev be scaled by *4 as well? Or transformed differently? Does the 1.96 make sense, or is that a variable (I was medium confused about that from the CLT assignment in the first place)
ci_lo <- df_q1$pi_est - 1.96 * df_q1$se
ci_hi <- df_q1$pi_est + 1.96 * df_q1$se
ci_lo
ci_hi
```

**Observations**:

- Does your interval include the true value of $\pi$?
  - Yes, if I calculated it correctly?
- Was your sample size $n$ large enough?
  - I guess so? I ran a few times and got values pretty close, with CI including the true value. When I tried with only 1000 or 10000 samples this was not the case.

# References
<!-- -------------------------------------------------- -->

[1] Dumoulin and Thouin, "A Ballistic Monte Carlo Approximation of Pi" (2014) ArXiv, [link](https://arxiv.org/abs/1404.1499)

[2] "How Mathematicians Used A Pump-Action Shotgun to Estimate Pi", [link](https://medium.com/the-physics-arxiv-blog/how-mathematicians-used-a-pump-action-shotgun-to-estimate-pi-c1eb776193ef)

[3] Art Owen "Monte Carlo", [link](https://statweb.stanford.edu/~owen/mc/)
